# Build Your First Agent Challenge - Part 1 Answers

## Task 1.1: Core Concepts ðŸ“š

### 1. What is a "node" in Aden's architecture? How does it differ from a traditional function?

A **node** in Aden is a unit of work in an agent graph that:
- Receives **context** (goal, shared memory, input)
- Makes **decisions** (using LLM, tools, or logic)
- Produces **results** (output, state changes)
- **Records everything** to the Runtime for observability

**Key differences from traditional functions:**
| Traditional Function | Aden Node |
|---------------------|-----------|
| Stateless execution | Access to SharedMemory for persistent state |
| No built-in logging | Automatic decision recording to Runtime |
| Manual error handling | Retry behavior with configurable `max_retries` |
| Direct calls | Graph-based execution with conditional edges |
| No LLM awareness | Built-in LLM provider access |
| Returns a value | Returns `NodeResult` with success/failure, output, routing decisions, and metrics |

Reference: [core/framework/graph/node.py](core/framework/graph/node.py) - `NodeSpec` and `NodeProtocol`

---

### 2. SDK-wrapped node concept - Four capabilities every node gets automatically:

According to the README and DEVELOPER docs, **every SDK-wrapped node gets**:

1. **Shared Memory** - Read/write access to `SharedMemory` for state management across nodes
2. **Local RLM Memory** - Long-term memory specific to the node/agent for learning
3. **Monitoring** - Automatic decision recording via `Runtime.decide()` and `record_outcome()`
4. **Tools + LLM Access** - Pre-configured access to available tools and LLM provider

This is delivered via `NodeContext` which provides:
```python
@dataclass
class NodeContext:
    runtime: Runtime           # For decision logging
    memory: SharedMemory       # For state
    llm: LLMProvider | None    # For generation
    available_tools: list[Tool] # For actions
    goal_context: str          # For guidance
```

Reference: [core/framework/graph/node.py#L421-L456](core/framework/graph/node.py)

---

### 3. Key differences:

#### Coding Agent vs Worker Agent:
| Coding Agent | Worker Agent |
|--------------|--------------|
| **Generates** specialized Worker Agents from natural language goals | **Executes** the actual tasks defined in the agent graph |
| Creates agent graphs, connection code, and test cases | Runs with SDK-wrapped nodes, tools, and observability |
| Operates during **build time** | Operates during **runtime** |
| Evolves agents when failures occur | Reports failures for the Coding Agent to analyze |

From README: "A Coding Agent generates specialized Worker Agents (Sales, Marketing, Ops) from natural language goals"

#### Goal-driven vs Workflow-driven development:
| Goal-Driven (Aden) | Workflow-Driven (Traditional) |
|--------------------|-------------------------------|
| Describe **outcomes** in natural language | Manually define **step-by-step workflows** |
| Framework generates agent graph automatically | Developer codes each agent interaction |
| Self-adapting when failures occur | Reactive error handling |
| Success criteria define completion | Explicit termination conditions coded |

#### Predefined edges vs Dynamic connections:
| Predefined Edges | Dynamic Connections |
|------------------|---------------------|
| Hardcoded in graph definition | Generated by LLM based on goals |
| Fixed workflow paths | Adaptable routing based on context |
| Must anticipate all scenarios | Emerges through iteration |
| Manual updates for new cases | Self-evolving on failure |

From README: "No predefined edges; connection code is generated by any capable LLM based on your goals"

---

### 4. Why does Aden generate "connection code" instead of using a fixed graph structure?

**Three key reasons:**

1. **Adaptability**: Fixed graphs can't adapt to new scenarios. Generated connection code can be evolved by the Coding Agent when agents fail, enabling self-improvement.

2. **Goal-driven flexibility**: When you describe outcomes instead of workflows, the optimal path between nodes depends on the specific goal. Connection code captures the reasoning for why nodes connect, not just that they do.

3. **Failure recovery**: From README: "When things break, the framework captures failure data, evolves the agent through the coding agent, and redeploys."

Fixed graphs require manual updates. Generated connection code can be automatically regenerated with improved logic.

Reference: [README.md](README.md) Features section

---

## Task 1.2: Memory Systems ðŸ§ 

### 1. Three types of memory:

#### Shared Memory (`SharedMemory`)
- **Scope**: Single graph execution (run-level)
- **Purpose**: Pass data between nodes in a workflow
- **Implementation**: Key-value store with read/write permissions per node
- **Features**: 
  - Thread-safe parallel writes via `write_async()` with per-key locking
  - Hallucination detection (rejects suspicious code-like content >5000 chars)
  - Permission scoping via `with_permissions()`

Reference: [core/framework/graph/node.py#L227-L398](core/framework/graph/node.py)

#### STM (Short-Term Memory)
- **Scope**: Session/execution context
- **Purpose**: State-based memory for current task context
- **Use case**: Maintaining conversation context, current task state, intermediate results
- **Characteristics**: Fast access, cleared after session ends

#### LTM/RLM (Long-Term Memory / Retrieval-based Long-term Memory)
- **Scope**: Persistent across executions
- **Purpose**: Learning from past decisions and outcomes
- **Use case**: 
  - Remembering successful patterns
  - Learning from rejected outputs
  - Building knowledge over time
- **Enables**: Self-improvement by correlating decisions with outcomes

Reference: ROADMAP.md shows "STM Layer Tool (state-based short-term memory)" and "LTM Layer Tool (RLM - long-term memory)" as implemented features.

---

### 2. When to use each type:

| Memory Type | Use When |
|-------------|----------|
| **Shared Memory** | Passing data between nodes in a single run (e.g., ticket content â†’ categorization â†’ response) |
| **STM** | Maintaining context within a session (e.g., multi-turn conversation, current task state) |
| **LTM/RLM** | Learning patterns across executions (e.g., which responses led to customer satisfaction, what prompts got rejected) |

---

### 3. Session Local memory isolation:

**How it works:**

The framework uses `SharedStateManager` with explicit **isolation levels**:

```python
class IsolationLevel(Enum):
    ISOLATED = "isolated"       # Private state per execution
    SHARED = "shared"           # Visible across executions (eventual consistency)
    SYNCHRONIZED = "synchronized"  # Shared with write locks (strong consistency)
```

**Session Local isolation** (`ISOLATED` level):
- Each execution gets its own memory namespace
- Writes in one execution don't affect others
- Prevents race conditions when multiple entry points trigger concurrently

Example from tests:
```python
mem1 = manager.create_memory("exec-1", "stream-1", IsolationLevel.ISOLATED)
mem2 = manager.create_memory("exec-2", "stream-1", IsolationLevel.ISOLATED)

await mem1.write("key", "value1")
await mem2.write("key", "value2")

# Each execution sees its own value
assert await mem1.read("key") == "value1"
assert await mem2.read("key") == "value2"
```

**Why this matters:** Real agents handle concurrent events (webhooks, API calls, timers). Without isolation, concurrent executions would overwrite each other's state.

Reference: [core/framework/runtime/tests/test_agent_runtime.py#L163-L173](core/framework/runtime/tests/test_agent_runtime.py), [docs/architecture/multi-entry-point-agents.md](docs/architecture/multi-entry-point-agents.md)

---

## Task 1.3: Human-in-the-Loop ðŸ™‹

### 1. What triggers a human intervention point?

Based on the HITL implementation, triggers include:

1. **Explicit `human_input` nodes** - Defined in the graph as `node_type: "human_input"`
2. **Confidence thresholds** - When agent confidence falls below `confidence_threshold`
3. **High-impact actions** - Actions requiring approval (financial transactions, publishing, deletions)
4. **Policy rules** - Configurable rules based on action type or context
5. **Missing information** - When the agent needs clarification to proceed

From `HITLRequest` in [core/framework/graph/hitl.py](core/framework/graph/hitl.py):
```python
@dataclass
class HITLRequest:
    objective: str          # What we're trying to accomplish
    current_state: str      # Where we are in the process
    questions: list[HITLQuestion]  # What we need to know
    missing_info: list[str] # Information gaps
```

---

### 2. What happens if a human doesn't respond within the timeout?

Based on the framework design:

1. **Configurable timeout** - Each HITL request has a `deadline` / `timeout` setting
2. **Escalation policy** - Defined via `escalation_policy` in config:
   - Escalate to another human (manager, on-call)
   - Use default/safe action
   - Abort the execution
   - Retry the request

From the HITL article example:
```python
class HITLAgent:
    def __init__(self, config):
        self.timeout = config.human_timeout
        self.escalation_policy = config.escalation
```

3. **Execution pauses** - `ExecutionResult.paused_at` tracks which node is waiting
4. **State preserved** - `session_state` in `ExecutionResult` enables resumption

Reference: [docs/articles/human-in-the-loop-ai-agents.md](docs/articles/human-in-the-loop-ai-agents.md)

---

### 3. Three scenarios where HITL would be essential:

1. **High-stakes decisions** - Financial transactions, legal document generation, medical advice
   - *Example*: Sales agent needs approval before offering >20% discount
   - *Why*: Irreversible actions with significant business impact

2. **Content publishing/external communication** - Emails to customers, blog posts, social media
   - *Example*: Marketing agent writes blog post, human approves before WordPress publish
   - *Why*: Brand reputation, compliance, legal liability

3. **Ambiguous or low-confidence situations** - Missing context, conflicting information
   - *Example*: Support ticket mentions both billing and technical issues, agent unsure which team to route to
   - *Why*: Better to ask than guess wrong

**Additional scenarios from the codebase:**
- Code review before deployment
- Creative content approval
- Customer escalation decisions
- Compliance-sensitive actions

Reference: [docs/articles/human-in-the-loop-ai-agents.md](docs/articles/human-in-the-loop-ai-agents.md) and [README.md](README.md)
